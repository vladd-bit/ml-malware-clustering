import collections
import datetime
import os
import warnings

import math
import numpy
import sklearn
import _pickle as cpickle

from matplotlib import pyplot
from sklearn import metrics

from src.errorLog import Log

def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn

#warnings.simplefilter(action='ignore', category=FutureWarning)


def split_subsets(input_vector, number_of_subsets=2):
    if type(input_vector) == dict:
        input_vector = list(input_vector.items())

    vector_len = len(input_vector)
    subset_size = math.floor(vector_len / number_of_subsets)

    subset_s = {}

    for i in range(0, number_of_subsets + 2):
        items = []

        if i * subset_size <= vector_len:
            for index in range((i - 1) * subset_size, i * subset_size):
                items.append(input_vector[index])
            subset_s[i] = items
            if len(items) > 0:
                subset_s[i] = items

        else:
            for index in range((i - 1) * subset_size, vector_len):
                items.append(input_vector[index])
            if len(items) > 0:
                subset_s[i] = items

            break

    return subset_s

"""
    Computes the scores for both labeled and unlabeled predictions
"""
def calculate_stats(algorithm_type="", true_labels=None, dataset=None, dataset_dense=None,
                    pred_labels=None, n_clusters=None, metric_used="euclidean"):

    algorithm_exceptions = ['gaussian-em', 'dbscan']

    num_distinct_labels = len(numpy.unique(pred_labels))

    log_data = {
        'algorithm_type': algorithm_type,

        'n_clusters': n_clusters,

        'precision_micro': metrics.precision_score(y_true=true_labels, y_pred=pred_labels, average='micro',
                                                   labels=numpy.unique(pred_labels)),
        'precision_macro': metrics.precision_score(y_true=true_labels, y_pred=pred_labels, average='macro',
                                                   labels=numpy.unique(pred_labels)),

        'accuracy_normalized': metrics.accuracy_score(y_true=true_labels, y_pred=pred_labels,
                                                      normalize=True),

        'purity_score': purity_score(y_true=true_labels, y_pred=pred_labels),

        'accuracy': metrics.accuracy_score(y_true=true_labels, y_pred=pred_labels,
                                           normalize=False),


        'balanced_accuracy_score': metrics.balanced_accuracy_score(y_true=true_labels, y_pred=pred_labels),

        'adjusted_mi': metrics.adjusted_mutual_info_score(labels_true=true_labels,
                                                          labels_pred=pred_labels, ),

        'normalized_mi': metrics.normalized_mutual_info_score(labels_true=true_labels,
                                                              labels_pred=pred_labels),

        'adjusted_rand_score': metrics.adjusted_rand_score(labels_true=true_labels, labels_pred=pred_labels),

        'recall_score_micro': metrics.recall_score(y_true=true_labels, y_pred=pred_labels, average='micro',
                                                   labels=numpy.unique(pred_labels)),

        'recall_score_macro': metrics.recall_score(y_true=true_labels, y_pred=pred_labels, average='macro',
                                                   labels=numpy.unique(pred_labels)),

        'f1_score_micro': metrics.f1_score(y_true=true_labels, y_pred=pred_labels, average='micro',
                                           labels=numpy.unique(pred_labels)),

        'f1_score_macro': metrics.f1_score(y_true=true_labels, y_pred=pred_labels, average='macro',
                                           labels=numpy.unique(pred_labels)),

        'completeness': sklearn.metrics.completeness_score(labels_true=true_labels,
                                                           labels_pred=pred_labels),

        'v_measure': sklearn.metrics.v_measure_score(labels_true=true_labels,
                                                     labels_pred=pred_labels),

        'homogeneity_score': metrics.homogeneity_score(labels_true=true_labels, labels_pred=pred_labels)
    }

    if num_distinct_labels > 1 and dataset_dense is not None:
        log_data.update(
            {'silhouette_score': str(sklearn.metrics.silhouette_score(dataset, pred_labels, metric=metric_used)),
             'davis_bouldin_score': str(sklearn.metrics.davies_bouldin_score(X=dataset_dense, labels=pred_labels))})
    else:
        log_data.update(
            {'silhouette_score': str(0.0),
             'davis_bouldin_score': str(0.0)})

    return log_data


def purity_score(y_true, y_pred):
    # compute contingency/confusion matrix
    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)
    return numpy.sum(numpy.amax(contingency_matrix, axis=0)) / numpy.sum(contingency_matrix)


def get_stats_by_algorithm(stats):
    stats_by_algorithm = {}

    for item in stats:
        for algorithm_type, vals in item.items():
            if type(vals) == list:
                vals = vals[0]
            for val in vals['stats']:
                if val['algorithm_type'] not in stats_by_algorithm.keys():
                    stats_by_algorithm[val['algorithm_type']] = [val]
                else:
                    stats_by_algorithm[val['algorithm_type']].append(val)

    return stats_by_algorithm


def recompute_stats(stats, dataset, dataset_dense, true_labels):

    all_stats = []

    for item in stats:
        new_stats_by_algorithm = {}
        for algoithm_type, vals in item.items():

            global_stats = {}
            substats = []
            predictions = []

            for i, val in enumerate(vals['stats']):
                if type(vals['predictions']) == list:
                    substats.append(calculate_stats(algorithm_type=vals['stats'][i]['algorithm_type'],
                                                    n_clusters=vals['stats'][i]['n_clusters'], dataset=dataset,
                                                    dataset_dense=dataset_dense,
                                                    true_labels=true_labels,
                                                    pred_labels=vals['predictions'][i]))
                else:
                    substats.append(calculate_stats(algorithm_type=vals['stats'][i]['algorithm_type'],
                                                    n_clusters=vals['stats'][i]['n_clusters'], dataset=dataset,
                                                    dataset_dense=dataset_dense,
                                                    true_labels=true_labels,
                                                    pred_labels=vals['predictions']))

            global_stats.update({'stats': substats, 'predictions': vals['predictions']})

            if algoithm_type not in new_stats_by_algorithm.keys():
                new_stats_by_algorithm[algoithm_type] = [global_stats]
            else:
                new_stats_by_algorithm[algoithm_type].append(global_stats)

        all_stats.extend([new_stats_by_algorithm])

    return all_stats


"""
    Gets the most important malware families according to the minimum number of samples it should contain,
    if the number of samples is negative then it picks the malware families that have less samples than the threshold.
"""
def get_most_important_malware_families(samples, sample_count_threshold=100):

    sample_count_by_family = {}

    for family, apk_num in samples.items():
        sample_count_by_family[family] = len(apk_num)

    sorted_samples = collections.OrderedDict(
        sorted(sample_count_by_family.items(), key=lambda kv: kv[1], reverse=True))

    threshold_samples = {}

    if sample_count_threshold < 0:
        for key, value in sorted_samples.items():
            if value <= abs(sample_count_threshold):
                threshold_samples[key] = value
    elif sample_count_threshold > 0:
        for key, value in sorted_samples.items():
            if value >= sample_count_threshold:
                threshold_samples[key] = value

    return {'sorted_samples': sorted_samples,
            'threshold_samples': threshold_samples,
            'sample_count_threshold': sample_count_threshold}

def log_stats(stats):
    from malwareClustering import export_stats_folder_name

    for item in stats:
        stats_by_algorithm = dict(item)
        for algorithm_type, vals in stats_by_algorithm.items():
            v = vals['stats']
            for i in v:
                for stat_name, stat_value in i.items():
                    Log.log_message(log_folder=export_stats_folder_name, log_level="INFO",
                                    log_message=stat_name + " : "
                                                + str(stat_value), log_file=export_stats_folder_name)

            Log.log_message(log_folder=export_stats_folder_name, log_level="INFO",
                            log_message="#####################################################",
                            log_file=export_stats_folder_name)


def serialize_data(data, file_name, ignore_timestamp=False):
    from malwareClustering import output_directory

    current_time = datetime.datetime.now()

    if ignore_timestamp:
        file_name = output_directory + file_name + '.bin'
    else:
        file_name = output_directory + file_name + '_' + str(current_time.year) \
                    + '_' + str(current_time.month) + '_' + str(current_time.day) + '_' + str(current_time.hour) \
                    + '_' + str(current_time.second) + '_' + str(current_time.microsecond) + '.bin'

    directory = os.path.dirname(file_name)
    if not os.path.exists(directory):
        os.makedirs(directory)

    with open(file_name, 'wb') as file:
        cpickle.dump(data, file)


def plot_data(dataset, cluster_centers=None, pred_labels=None, labels=None, label_col_names=None):
    if len(dataset[0]) == 2:
        pyplot.subplot(1, 1, 1)
        pyplot.scatter(numpy.array(dataset[:, 0]), numpy.array(dataset[:, 1]), s=1, c=pred_labels, cmap='rainbow')

        print(cluster_centers)
        pyplot.scatter(cluster_centers[:, 0], cluster_centers[:, 1],
                       marker='x', linewidths=1, color='blue')

        if label_col_names is not None:
            texts = list(label_col_names)
            for index, centroid_coords in enumerate(cluster_centers):
                pyplot.annotate(s=texts[index], xy=centroid_coords)

        pyplot.xlabel('malware families', fontsize=8)
        pyplot.ylabel('malware families', fontsize=8)

        pyplot.show()
    else:
        figure = pyplot.subplots(squeeze=False)
        axis = []
        data_dimensions = len(dataset[0])
        for i in range(0, data_dimensions):
            for j in range(i + 1, data_dimensions):
                axis.append(pyplot.subplot2grid((data_dimensions, data_dimensions), (i, j)))
                axis[-1].scatter(cluster_centers[:, i], cluster_centers[:, j], linewidths=1, marker='x', cmap='red')
                # if label_col_names is not None:
                #    texts = list(label_col_names)
                #    for index, centroid_coords in enumerate(cluster_centers[:, [i,j]]):
                #        axis[-1].annotate(s=texts[index], xy=centroid_coords)

        pyplot.show()
