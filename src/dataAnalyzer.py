import collections

import warnings

warnings.filterwarnings("ignore")

import pandas
import numpy
from IPython.core.pylabtools import figsize
from mpl_toolkits import axisartist
from mpl_toolkits.axes_grid1 import host_subplot

from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.feature_extraction import DictVectorizer
from matplotlib import pyplot, ticker

from src.util import get_stats_by_algorithm, get_most_important_malware_families
from sklearn.manifold import TSNE


class DataAnalyzer:

    def __init__(self):
        self.vectorized_data = {}
        self.truncated_data = {}
        self.truncated_data_pca = {}

    """
        Performs one-hot encoding on the feature vector of strings, returning a sparse matrix.
        Also performs PCA and SVD afterwards so that they are stored in for future use across the program
        as they are stored in the class object.
        Inputs: -raw feature vector of strings from the APK feature extraction.
                -number of components for SVD and PCA
        Returns: -
    """

    def record_feature_presence(self, feature_vectors, truncated_svd_components=2, pca_components=2,
                                feature_reduction=True):

        dict_vectorizer = DictVectorizer(sparse=True)
        data = dict_vectorizer.fit_transform(list(feature_vectors.values()))
        self.vectorized_data = data

        print("Number of features:", numpy.shape(self.vectorized_data)[1])
        print("Features variance retained compared to full dataset:", (numpy.shape(self.vectorized_data)[1]
                                                                       * 100 / 10677))
        print("Standard deviation no feature reduction: ", numpy.std(data.toarray()))
        print("Variance no feature reduction ", numpy.var(data.toarray()))

        if feature_reduction:
            truncated_data = TruncatedSVD(n_components=truncated_svd_components).fit_transform(data)
            self.truncated_data = truncated_data

            print("Standard deviation SVD: ", numpy.std(self.truncated_data))
            print("Variance SVD: ", numpy.var(self.truncated_data))

            pca = PCA(n_components=pca_components)
            self.truncated_data_pca = pca.fit_transform(data.toarray())
            print("Explained Variance:", pca.explained_variance_)
            print("Explained Variance_ratio: ", pca.explained_variance_ratio_)

            print("Noise_variance: ", pca.noise_variance_)
            print("Standard deviation PCA: ", numpy.std(self.truncated_data_pca))
            print("Variance PCA ", numpy.var(self.truncated_data_pca))

    """
        Function that plots the malware families according to the number of samples.
    """

    def plot_sample_data_histogram(self, samples, full_samples, sample_threshold=-1,
                                   downsample_threshold=-1):

        st_samples = get_most_important_malware_families(samples, sample_threshold)

        original_full_samples = get_most_important_malware_families(full_samples)

        data = list(original_full_samples['sorted_samples'].values())
        x_axis_names = list(original_full_samples['sorted_samples'].keys())
        x = numpy.arange(len(data))

        th_data = list(st_samples['threshold_samples'].values())
        th_x_axis_names = list(st_samples['threshold_samples'].keys())
        th_x = numpy.arange(len(th_data))

        fontdict = {'fontsize': 16}

        fig = pyplot.figure(figsize=(20, 15))

        pyplot.subplot(1, 1, 1)
        pyplot.style.use('ggplot')

        #
        pyplot.xlabel('Family names', fontdict=fontdict)
        pyplot.ylabel('Number of samples', fontdict=fontdict)
        pyplot.title('Significant malware families distribution, min number of samples threshold '
                     + str(st_samples['sample_count_threshold']), fontdict=fontdict)
        #
        bar_rects1 = pyplot.bar(th_x, height=th_data, label="")
        #
        ax1 = pyplot.gca()
        ax1.set_xticks(th_x)
        ax1.set_xticklabels(th_x_axis_names, fontdict=fontdict, rotation=40, rotation_mode="anchor", ha="right")

        self.label_bars(bar_rects1, ax1)

        from malwareClustering import output_directory

        if downsample_threshold > 0:
            pyplot.savefig(output_directory + 'threshold_dataset_' + str(st_samples['sample_count_threshold'])
                           + '_downsample_' + str(downsample_threshold) + '.eps',
                           format='eps', dpi=3000)
        else:

            pyplot.savefig(output_directory + 'threshold_dataset_' + str(st_samples['sample_count_threshold']) + '.eps',
                           format='eps', dpi=3000)

        fig.clear()
        pyplot.close(fig)

    def dataset_visualisation(self, label_numbers, family_label_numbers, dataset=None,
                              sample_threshold=0, downsample_threshold=0):

        if dataset is None:
            dataset = self.vectorized_data

        labels_num_unique = numpy.unique(label_numbers)
        family_label_ticks = []

        total_num_labels = len(numpy.unique(label_numbers))

        for i in labels_num_unique:
            for family_name, family_id in family_label_numbers.items():
                if i == family_id:
                    family_label_ticks.append(family_name)

        tsne = TSNE(n_components=2, n_iter=300, verbose=1, metric="cosine")
        results = tsne.fit_transform(dataset.toarray())

        fig = pyplot.figure(figsize=(16, 10))

        pyplot.scatter(results[:, 0], results[:, 1], c=label_numbers,
                       cmap=pyplot.cm.get_cmap("jet", len(numpy.unique(label_numbers))))

        pyplot.xlabel('Data point 1st component')
        pyplot.ylabel('Data point 2nd component')

        cb = pyplot.colorbar(ticks=range(0, len(numpy.unique(label_numbers))))
        cb.ax.set_yticklabels(family_label_ticks)
        cb.set_label('Malware class/family colours')

        if total_num_labels > 16:
            tick_locator = ticker.MaxNLocator(nbins=20)
            cb.locator = tick_locator
            cb.update_ticks()

        fig.canvas.set_window_title("TSNE")

        pyplot.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=1.5)

        from malwareClustering import output_directory
        pyplot.savefig(output_directory + "data_visualization_TSNE_" + "_sample_thershold_"
                       + str(sample_threshold) + "_downsampled_" + str(downsample_threshold) + '.eps',
                       format='eps', dpi=2000)

        fig.clear()
        pyplot.close(fig)

        pca_2d = PCA(n_components=2)
        pca_results_2d = pca_2d.fit_transform(dataset.toarray())

        print("Explained Variance 2D:", pca_2d.explained_variance_)
        print("Explained Variance_ratio 2D: ", pca_2d.explained_variance_ratio_)

        print("Noise_variance 2D: ", pca_2d.noise_variance_)
        print("Standard deviation PCA 2D: ", numpy.std(pca_results_2d))
        print("Variance PCA 2D ", numpy.var(pca_results_2d))

        pca_results_3d = PCA(n_components=3).fit_transform(dataset.toarray())

        fig = pyplot.figure(figsize=(16, 10))

        pyplot.scatter(pca_results_2d[:, 0], pca_results_2d[:, 1], c=label_numbers,
                       cmap=pyplot.cm.get_cmap("jet", len(numpy.unique(label_numbers))))

        pyplot.xlabel('Data point 1st component')
        pyplot.ylabel('Data point 2nd component')

        cb = pyplot.colorbar(ticks=range(0, len(numpy.unique(label_numbers))))
        cb.ax.set_yticklabels(family_label_ticks)
        cb.set_label('Malware class/family colours')

        fig.canvas.set_window_title("PCA 2d")
        if total_num_labels > 16:
            tick_locator = ticker.MaxNLocator(nbins=20)
            cb.locator = tick_locator
            cb.update_ticks()

        pyplot.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)
        pyplot.savefig(output_directory + "data_visualization_PCA_2D" + "_sample_thershold_"
                       + str(sample_threshold) + "_downsampled_" + str(downsample_threshold) + '.eps',
                       format='eps', dpi=2000)

        fig.clear()
        pyplot.close(fig)

        fig = pyplot.figure(figsize=(16, 10))
        ax = fig.gca(projection='3d')
        ax.scatter(
            xs=pca_results_3d[:, 0],
            ys=pca_results_3d[:, 1],
            zs=pca_results_3d[:, 2],
            c=label_numbers,
            cmap='tab10'
        )
        ax.set_xlabel('one')
        ax.set_ylabel('two')
        ax.set_zlabel('three')

        if total_num_labels > 15:
            tick_locator = ticker.MaxNLocator(nbins=20)
            cb.locator = tick_locator
            cb.update_ticks()

        pyplot.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)
        pyplot.savefig(output_directory + "data_visualization_PCA_3D" + "_sample_thershold_"
                       + str(sample_threshold) + "_downsampled_" + str(downsample_threshold) + '.eps',
                       format='eps', dpi=2000)

        fig.clear()
        pyplot.close(fig)

    @staticmethod
    def plot_stats(stats, sample_threshold=1):

        stats_by_algorithm = get_stats_by_algorithm(stats)

        ignore_stats = ['algorithm_type', 'n_clusters', 'confusion_matrix', 'multilabel_confusion_matrix']
        considered_columns = ['accuracy_normalized', 'adjusted_rand_score',
                              'f1_score', 'recall_score', 'normalized_mi', 'homogeneity_score', 'silhouette_score',
                              'davis_bouldin_score']

        fontdict = {'fontsize': 6}

        for algorithm_type in stats_by_algorithm.keys():

            data = pandas.DataFrame.from_records(stats_by_algorithm[algorithm_type])

            desired_metrics = list(data.head())

            fig, ax = pyplot.subplots(nrows=3, ncols=3, figsize=(20, 20))

            counter = 0
            row_num = 0

            for row in ax:
                col_num = 0

                if counter >= len(desired_metrics):
                    break

                for col in row:

                    ax[row_num][col_num].yaxis.set_major_formatter(ticker.FormatStrFormatter('%.3f'))
                    ax[row_num][col_num].set_xlabel('number of clusters', fontdict=fontdict)
                    ax[row_num][col_num].set_ylabel('value', fontdict=fontdict)

                    for i in range(counter, len(desired_metrics)):
                        metric_name = desired_metrics[i]

                        if metric_name not in ignore_stats and metric_name in considered_columns:
                            c = data[metric_name].astype(float).fillna(0.0)
                            ax[row_num][col_num].set_title(metric_name)
                            ax[row_num][col_num].plot(data['n_clusters'], c, color='red'),
                            counter = i
                            break

                    counter = counter + 1
                    col_num = col_num + 1
                    fig.canvas.set_window_title(algorithm_type)

                row_num = row_num + 1

            from malwareClustering import output_directory

            pyplot.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)
            pyplot.savefig(output_directory + "threshold_" + str(sample_threshold) + "_" +
                           algorithm_type.replace('|', '_') + '.eps',
                           format='eps', dpi=2000)
            fig.clear()
            pyplot.close(fig)

    @staticmethod
    def plot_important_metrics(stats, sample_threshold=1):

        ignore_stats = ['algorithm_type', 'n_clusters', 'confusion_matrix', 'multilabel_confusion_matrix']
        # considered_columns = ['algorithm_type', 'n_clusters', 'accuracy_normalized', 'adjusted_rand_score',
        #                      'f1_score_micro', 'recall_score_micro', 'normalized_mi', 'homogeneity_score', 'silhouette_score',
        #                      'davis_bouldin_score']

        considered_columns = ['algorithm_type', 'n_clusters', 'accuracy_normalized', 'adjusted_rand_score',
                              'f1_score_micro', 'normalized_mi', 'homogeneity_score', 'silhouette_score',
                              'davis_bouldin_score', 'purity_score']

        stats_by_algorithm = get_stats_by_algorithm(stats)

        colors = ['blue', 'green', 'red', 'darkviolet', 'magenta', 'indigo', 'lightblue', 'orange']

        fontdict = {'fontsize': 12}

        for algorithm_type in stats_by_algorithm.keys():

            color_index = 0

            data = pandas.DataFrame.from_records(stats_by_algorithm[algorithm_type])

            data_head = list(data.head())

            columns_found = []

            host = host_subplot(111, axes_class=axisartist.Axes, figure=figsize(25, 10))
            host.set_title(algorithm_type)
            # pyplot.subplots_adjust(right=0.10)

            for column_name in data_head:
                if column_name not in considered_columns:
                    del data[column_name]
                else:
                    columns_found.append(column_name)
                    if column_name not in ignore_stats:
                        data[column_name] = data[column_name].astype(float)

            # data = data.drop_duplicates(subset='n_clusters', keep='last')
            # data = data.sort_values('n_clusters')

            num_x_ticks = (data['n_clusters'].astype(int)).tolist()

            axis = []

            offset = 60

            host.set_yticklabels([])
            host.set_xticks([])
            host.set_xlabel("number of clusters")

            for column_name in columns_found:
                if column_name not in ignore_stats:
                    tmp_ax = host.twinx()
                    #
                    new_fixed_axis = tmp_ax.get_grid_helper().new_fixed_axis
                    tmp_ax.axis["right"] = new_fixed_axis(loc="right",
                                                          axes=tmp_ax,
                                                          offset=(offset, 0))
                    tmp_ax.axis["right"].toggle(all=True)
                    tmp_ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.3f'))

                    tmp_ax.set_ylabel(column_name)

                    tmp_ax.plot(data[column_name], color=colors[color_index])

                    tmp_ax.axis["right"].label.set_color(colors[color_index])

                    color_index = color_index + 1
                    tmp_ax.set_label(column_name)

                    tmp_ax.set_xticks(num_x_ticks)

                    offset = offset + 60
                    axis.append(tmp_ax)

            if len(num_x_ticks) > 25:
                n = 30  #
                true_ticks = [i for i in num_x_ticks if i % n == 0 or i == 2 or i == (num_x_ticks[-1])]

                host.set_xticks(true_ticks)
                host.set_xticklabels([str(i) for i in true_ticks])
            else:
                # if algorithm_type == 'dbscan':
                host.set_xticks(range(0, len(num_x_ticks)))
                host.set_xticklabels([str(i) for i in num_x_ticks])

            host.legend([col_name for col_name in columns_found if col_name not in ignore_stats], loc='best')

            #
            # fig, ax1 = pyplot.subplots()
            # ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.3f'))
            # ax.set_xlabel('number of clusters')
            # ax.set_xticks(num_x_ticks)
            # ax.get_yaxis().set_visible(False)
            #
            # if len(num_x_ticks) > 25:
            #    # ax1.xaxis.set_major_locator(MaxNLocator(integer=True))
            #    n = 10  #
            #    [l.set_visible(False)
            #     for (i, l) in enumerate(ax1.xaxis.get_ticklabels()) if i % n != 0 and i != (len(num_x_ticks) - 1)]
            #
            #    #[l.set_visible(False)
            #    # for (i, l) in enumerate(ax1.yaxis.get_ticklabels()) if i % n != 0 and i != (len(num_x_ticks) - 1)]

            # ax1.set_ylabel('precision')
            # ax1.plot(t, data1, color=color)
            # ax1.tick_params(axis='y', labelcolor=color)

            # for column in columns_found:
            #    if column not in ignore_stats:
            #        tmp_ax = ax1.twinx()
            #
            #        tmp_ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.3f'))
            #        pyplot.plot(data[column].astype(float))
            #
            #        axis.append(tmp_ax)

            # fig.canvas.set_window_title(algorithm_type)

            pyplot.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)
            pyplot.tight_layout()
            pyplot.draw()

            from malwareClustering import output_directory
            pyplot.savefig(output_directory + "threshold_" + str(sample_threshold) + "_" +
                           algorithm_type.replace('|', '_') + '_stats_merged.eps',
                           format='eps', dpi=4000)
            host.cla()
            host.clear()
            pyplot.cla()
            pyplot.clf()
            pyplot.close()

            # means = [data['normalized_mi'],
            #         data['accuracy_normalized'],
            #         data['silhouette_score'] if 'silhouette_score' in desired_metrics else "0.0",
            #         data['homogeneity_score'],
            #         data['f1_score_micro'],
            #         data['recall_score_micro'],
            #         # data['adjusted_rand_score']
            #         ]

            width = 0.35
            # for i in means:
            #    pyplot.plot(data['n_clusters'], i)
            # pyplot.plot((1,2,3,4,5), (4,5,6,7,8))
            # pyplot.bar(ind, means, width)

            # pyplot.ylabel(str(algorithm_type))

            # pyplot.title(str(algorithm_type))

            # pyplot.xticks(ind, col_names)

    #
    #
    #

    #

    #

    @staticmethod
    def export_stats_to_csv(stats, sample_threshold):
        # considered_columns = ['algorithm_type', 'n_clusters', 'accuracy_normalized', 'adjusted_rand_score',
        #                      'f1_score_micro', 'f1_score_macro', 'recall_score_micro', 'recall_score_macro',
        #                      'precision_micro', 'precision_macro',
        #                      'normalized_mi', 'adjusted_mi', 'v_measure', 'completeness', 'purity_score',
        #                      'homogeneity_score',
        #                      'silhouette_score']

        considered_columns = ['algorithm_type', 'n_clusters',
                              'accuracy_normalized',
                              'adjusted_rand_score',
                              'davis_bouldin_score',
                              'f1_score_micro',
                              'normalized_mi',
                              'purity_score',
                              'homogeneity_score',
                              'silhouette_score']

        csv_data_new = pandas.DataFrame(columns=considered_columns)

        csv_data_best_values = pandas.DataFrame(columns=considered_columns)

        stats_by_algorithm = get_stats_by_algorithm(stats)
        for algorithm_type in stats_by_algorithm.keys():
            data = pandas.DataFrame.from_records(stats_by_algorithm[algorithm_type])
            for column_name in list(data.head()):
                if column_name not in considered_columns:
                    del data[column_name]
                else:
                    if column_name not in ('algorithm_type', 'n_clusters'):
                        data[column_name] = data[column_name].astype(float).round(4)
                        # data[column_name] = data[column_name].apply(lambda x: pandas.Series.round(x, 4))

            csv_data_new = csv_data_new.append(data.loc[:, considered_columns], ignore_index=True)
            # csv_data_new = csv_data_new.append(data[data['n_clusters'] == data['n_clusters'].max()].iloc[0],
            #                                   ignore_index=False)
            # csv_data_new = csv_data_new.append(data[data['n_clusters'] == data['n_clusters'].min()].iloc[0],
            #                                   ignore_index=False)

            if 'silhouette_score' in list(data.head()):
                csv_data_best_values = csv_data_best_values.append(
                    data[data['silhouette_score'] == data['silhouette_score'].max()].iloc[0], ignore_index=False)
            # csv_data_best_values = csv_data_best_values.append(
            #    data[data['normalized_mi'] == data['normalized_mi'].max()].iloc[0], ignore_index=False)
            # csv_data_best_values = csv_data_best_values.append(
            #    data[data['homogeneity_score'] == data['homogeneity_score'].max()].iloc[0], ignore_index=False)
            # csv_data_best_values = csv_data_best_values.append(
            #    data[data['accuracy_normalized'] == data['accuracy_normalized'].max()].iloc[0], ignore_index=False)
            # csv_data_best_values = csv_data_best_values.append(
            #    data[data['adjusted_rand_score'] == data['adjusted_rand_score'].max()].iloc[0], ignore_index=False)

        from malwareClustering import output_directory
        csv_data_new.to_csv(output_directory + "_all_min_max" + "_threshold_" + str(sample_threshold) + ".csv",
                            sep='\t',
                            encoding='utf-8', index=False)
        csv_data_new.to_latex(output_directory + "_all_min_max" + "_threshold_" + str(sample_threshold) + ".tex",
                              encoding='utf-8', index=False)

        csv_data_best_values.to_csv(
            output_directory + "_all_best_values" + "_threshold_" + str(sample_threshold) + ".csv",
            sep='\t',
            encoding='utf-8')

        # desired_metrics = list(data.head())
        # means = [str(max(data['normalized_mi'])),
        #         str(max(data['accuracy_normalized'])),
        #         str(max(data['silhouette_score'] if 'silhouette_score' in desired_metrics else "0.0")),
        #         str(max(data['homogeneity_score'])),
        #         str(max(data['f1_score'])),mukey
        #         str(max(data['recall_score'])),
        #         str(max(data['adjusted_rand_score']))
        #         ]

    #
    # for column_name in list(data.head()):
    #    if column_name not in considered_columns:
    #        del data[column_name]

    # from malwareClustering import output_directory
    # data.to_csv(output_directory + algorithm_type + "_threshold_" + str(sample_threshold) + ".csv",
    #            sep='\t',
    #            encoding='utf-8')

    @staticmethod
    def label_bars(rects, ax, fontdict=None):
        """
        Attach a text label above each bar displaying its height
        """
        for rect in rects:
            height = rect.get_height()
            ax.text(rect.get_x() + rect.get_width() / 2., 1.01 * height,
                    '%d' % int(height),
                    ha='center', va='bottom', fontdict=fontdict)
